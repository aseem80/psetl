#Spark Configuration
--spark.master.url=${SPARK_MASTER_URL:local[4]}
spark.master.url=spark://172.22.92.23:7077
spark.app.name=PS ETL
spark.home=${SPARK_HOME:/Users/bmwi/Documents/software/spark-1.6.3-bin-hadoop2.4}
spark.shuffle.service.enabled=false
spark.dynamicAllocation.enabled=false
spark.io.compression.codec=snappy
spark.rdd.compress=true
--spark.driver.host=172.22.89.139
--spark.driver.port=5050
spark.submit.deployMode=client
spark.rpc.netty.dispatcher.numThreads=2
spark.


#Spark SQL properties
--spark.sql.parquet.cacheMetadata=true

# Perf env
---spark.cassandra.connection.host=${CASSANDRA_HOST:172.22.74.154}
---spark.cassandra.auth.username=cassandra
---spark.cassandra.auth.password=prod123

### SIT env
spark.cassandra.connection.host=${CASSANDRA_HOST:172.22.70.160}
spark.cassandra.auth.username=cassandra
spark.cassandra.auth.password=cassandra

--spark.cassandra.connection.host=localhost
--spark.cassandra.output.consistency.level=LOCAL_ONE
--spark.cassandra.connection.timeout_ms=20000
--spark.cassandra.connection.keep_alive_ms=3000
#Approx amount of data to be fetched into a Spark partition. This value is in bytes. Property name is misleading
--spark.cassandra.input.split.size_in_mb=67108864
#Number of millisecons spark will wait for cassandra to respond
--spark.cassandra.read.timeout_ms=240000
#Minimum period of time to wait before reconnecting to a dead node
--spark.cassandra.connection.reconnection_delay_ms.min=10000




ps.etl.tasks.jar=${PS_ETL_TASKS_JAR:/Users/bmwi/ps/spark/lib/product-data-tasks.jar}


ps.etl.dropShipFileCronTabExpression=${PS_ETL_CRONTAB_EXPRESSION:0 0 0 * * ?}

ps.etl.dropShipFile.output.partition=10


ps.etl.dropShipEligibleFileLocation.prefix=${S3_LOCATION:s3a://rsp-nps-team/non-prod/spark/demo_sku_dropShipEligible_}
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
fs.s3a.proxy.host=proxy.pc.aws.cloud.nordstrom.net
fs.s3a.proxy.port=3128